# M1: Whisperバックエンド（最小API）

Whisperを使って音声ファイルを翻訳する最小構成のFastAPIアプリを作りたいです。

要件は以下の通りです：
- FastAPIを使用。
- エンドポイント `/translate` をPOSTで受け取り、音声ファイル（WAV/MP3）を受け取る。
- Whisperの "small" モデルをロードして、受信した音声を翻訳する。
- `task="translate"` を指定して、日本語→英語の翻訳を行う。
- 結果をJSON形式で返す。例：`{"translation": "translated text"}`
- 外部APIは使用しない（openai-whisperのみ使用）。
- 動作例：`curl -X POST -F "file=@sample.wav" http://localhost:8000/translate`
- 依存関係: `fastapi`, `uvicorn`, `whisper`
- Python 3.10+で動作。

出力形式：
- ファイル名: `main.py`
- Whisperのモデルロード・翻訳処理・FastAPIルート定義を含む。
- 主要処理にコメントを付けてください。

# M2: WebSocket通信でリアルタイム翻訳

M1で作成したFastAPIアプリを拡張して、WebSocket経由で音声をリアルタイム翻訳できるようにします。

要件：
- WebSocketエンドポイント `/ws` を作成。
- クライアントから2秒ごとに音声データ（バイト列）を送信できるようにする。
- サーバ起動時にWhisperモデルをロードしてキャッシュ。
- 各チャンク受信時に翻訳を行い、翻訳結果をWebSocket経由で送信。
- Whisperの `task="translate"` を利用して日本語→英語翻訳。
- 音声処理部分は非同期（async）関数で書いてください。
- JSON形式で翻訳結果を返す（例：`{"text": "translated text"}`）。

出力形式：
- ファイル名: `main.py`
- WebSocket接続の処理フローを明確にコメント付きで書いてください。
- M1との互換性を保ってください。

# M3: フロントエンドの構築（HTML + JS）

M2のWebSocketサーバに接続して、マイク音声を送信・翻訳結果を字幕表示するWebページを作りたいです。

要件：
- HTML + JavaScriptのみで作成（フレームワーク不要）。
- 「Start Translation」ボタンを設置。
- `MediaRecorder` APIを使用してマイク音声を録音。
- 約2秒ごとに音声データをWebSocketで送信。
- サーバから受け取った翻訳結果をリアルタイムに `<div id="subtitle">` に表示。
- PDFスライドを下部に表示（pdf.js使用）。
- 字幕テキストは画面下部中央に白文字＋黒シャドウで重ねる。

出力形式：
- ファイル名: `index.html`
- 実行すれば即動く形。
- 主要箇所にコメントを付けてください。
